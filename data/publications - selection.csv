ID,Authors,Title,Year,Source title,Cited by,DOI,Abstract
P003,"E.C., Groen, Eduard C.; F., Dalpiaz, Fabiano; M., van Vliet, Martijn; B., Winter, Boris; J., Doerr, Joerg; S., Brinkkemper, Sjaak","Classification of quality characteristics in online user feedback using linguistic analysis, crowdsourcing and LLMs",2025,Journal of Systems and Software,0,10.1016/j.jss.2025.112533,"Software qualities such as usability or reliability are among the strongest determinants of mobile app user satisfaction and constitute a significant portion of online user feedback on software products, making it a valuable source of quality-related feedback to guide the development process. The abundance of online user feedback warrants the automated identification of quality characteristics, but the online user feedback's heterogeneity and the lack of appropriate training corpora limit the applicability of supervised machine learning. We therefore investigate the viability of three approaches that could be effective in low-data settings: language patterns (LPs) based on quality-related keywords, instructions for crowdsourced micro-tasks, and large language model (LLM) prompts. We determined the feasibility of each approach and then compared their accuracy. For the complex multiclass classification of quality characteristics, the LP-based approach achieved a varied precision (0.38–0.92) depending on the quality characteristic, and low recall; crowdsourcing achieved the best average accuracy in two consecutive phases (0.63, 0.72), which could be matched by the best-performing LLM condition (0.66) and a prediction based on the LLMs’ majority vote (0.68). Our findings show that in this low-data setting, the two approaches that use crowdsourcing or LLMs instead of involving experts achieved accurate classifications, while the LP-based approach had only limited potential. The promise of crowdsourcing and LLMs in this context might even extend to building training corpora. © 2025 Elsevier B.V., All rights reserved."
P005,"S., Saleem, Summra; M.N., Asim, Muhammad Nabeel; A.R., Dengel, Andreas R.",PassionNet: An innovative framework for duplicate and conflicting requirements identification,2025,Expert Systems with Applications,0,10.1016/j.eswa.2025.128684,"Early detection of duplicate and conflicting requirements in software development lifecycle is crucial to achieve software project efficiency, quality, and market success. Primarily, duplicate detection requires identifying semantic equivalence, intent alignment, functional overlap, and domain-specific terminology variations between differently worded requirements. Whereas, conflict detection demands recognising logical contradictions, constraint violations, resource conflicts and temporal incompatibilities between requirements. To handle multi-dimensional demands of two different task types, researchers have developed 32 AI based duplicate and conflicting requirement detection predictors. However, despite the utility of sophisticated large language models (LLMs) and sampling techniques, existing approaches significantly lack in performance because they fail to comprehensively handle multi-dimensional demands of both tasks. To address these gaps, this paper presents a modular framework “PassionNet” which implements a novel strategy of integrating 10 different multi-dimensional similarity assessments with the contextual understanding of 8 unique language model variants. The framework enables three distinct pipeline types: language model-based pipelines that capture semantic intent, similarity knowledge-driven pipelines that detect lexical, structural and distributional patterns, and hybrid pipelines that combine both approaches to simultaneously assess all dimensions of requirement relationships. Our experimental evaluation of 760 pipelines across six public datasets demonstrates that hybrid pipelines outperform the other two approaches in terms of F1-score as compared to state-of-the-art methods. Specifically, the hybrid pipeline achieves an improvement in F1-score of approximately 4 % on the WorldVista dataset, 5 % on the UAV dataset and 3 % on the Pure dataset as compared to the state-of-the-art models. Statistical validation through t-tests confirms the significance of these improvements (p < 0.1 with 10 permutations, approaching zero with 1000 permutations). The results provide empirical evidence that effective requirement analysis requires simultaneously assessing semantic, lexical, structural, and logical dimensions of requirements rather than focusing on isolated aspects. To facilitate software engineers, researchers and practitioners, PassionNet web application is deployed at https://sds_requirement_engineering.opendfki.de/. © 2025 Elsevier B.V., All rights reserved."
P009,"L., Yang, Longxing; Y., Luo, Yixing; H., Gao, Hao; Y., Fan, Yingshuang; J., Zhang, Jingru; X., Li, Xiaofeng; X., Dong, Xiaogang; B., Gu, Bin; Z., Jin, Zhi; M., Yang, Mengfei",Evaluating Large Language Models for Requirements Question Answering in Industrial Aerospace Software,2025,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,0,10.1145/3696630.3728560,"Aerospace software presents significant challenges to requirements engineering due to its design complexity and stringent safety standards. When manually drafting requirement documents, engineers need strong domain knowledge while also navigating heterogeneous data, which leads to errors and inefficiencies. This paper evaluates the capabilities of large language models (LLMs) in understanding aerospace software requirements and their potential to assist in requirements question answering (QA). We develop an aerospace requirements QA benchmark based on industrial software assets, books, and research materials, creating a total of 6, 696 QA pairs across ten tasks and three heterogeneous data formats: text, tables, and formulas. We then evaluate the domain-specific performance of five mainstream open-source LLMs using zero-shot learning, few-shot learning, and retrieval-augmented generation (RAG) techniques. We further categorize hallucinations from LLMs and quantitatively analyze error distributions. Moreover, we conduct a user study to assess the LLM’s practical usefulness when applying to requirements QA. The evaluation results show that (1) LLMs demonstrate limited performance in the aerospace software domain, (2) RAG techniques significantly enhance the capabilities of LLMs for text-based tasks, while few-shot learning improves the performance of most LLMs, (3) four distinct types of QA hallucinations are identified, and (4) LLM QA is particularly beneficial for junior engineers. This research provides valuable perspectives for the future application of LLMs in aerospace software. © 2025 Elsevier B.V., All rights reserved."
P013,"M., Bragilovski, Maxim; A.T., van Can, Ashley T.; F., Dalpiaz, Fabiano; A., Sturm, Arnon",Leveraging machines to derive domain models from user stories,2025,Requirements Engineering,0,10.1007/s00766-025-00442-9,"Domain models play a crucial role in software development, as they provide means for communication among stakeholders, for eliciting requirements, and for representing the information structure behind a database scheme or for model-driven development. However, creating such models is a tedious activity and automated support may assist in obtaining an initial domain model that can later be enriched by human analysts. In this paper, we compare the effectiveness of various approaches for deriving domain models from a given set of user stories. We contrast human derivation (of both experts and novices) with machine derivation; for the latter, we compare (i) the Visual Narrator: an existing rule-based NLP approach; (ii) a machine learning classifier that we feature engineered; and (iii) a generative AI approach that we constructed via prompt engineering with multiple configurations. Based on a benchmark dataset comprising nine collections of user stories and their corresponding domain models, the evaluation shows that while no approach matches human performance, large language models (LLMs) are not statistically outperformed by human experts in deriving classes. Additionally, a tuned version of the machine learning approach achieves results close to human performance in deriving associations. To better understand the results, we qualitatively analyze them and identify differences in the types of false positives as well as other factors that affect performance. © 2025 Elsevier B.V., All rights reserved."
P017,"J., Zhang, Jianzhang; J., Zhou, Jialong; J., Hua, Jinping; N., Niu, Nan; C., Liu, Chuang",Mining user privacy concern topics from app reviews,2025,Journal of Systems and Software,1,10.1016/j.jss.2025.112355,"Context: As mobile applications (apps) widely spread throughout our society and daily life, various personal information is constantly demanded by apps in exchange for more intelligent and customized functionality. An increasing number of users are voicing their privacy concerns through app reviews on app stores. Objective: The main challenge of effectively mining privacy concerns from user reviews lies in that reviews expressing privacy concerns are overridden by a large number of reviews expressing more generic themes and noisy content. In this work, we propose a novel automated approach to overcome that challenge. Method: Our approach first employs information retrieval and document embeddings to extract candidate privacy reviews in an unsupervised manner, which are further labeled to prepare the annotation dataset. Then, supervised classifiers are trained to automatically identify privacy reviews. Finally, an interpretable topic mining algorithm is designed to detect privacy concern topics contained in the privacy reviews. Results: Experimental results show that the best performing document embedding achieves an average precision of 96.80% in the top 100 retrieved candidate privacy reviews, outperforming the taxonomy-based baseline, which achieves 73.87%. All trained privacy review classifiers achieve an F<inf>1</inf> score above 91%, surpassing the keyword-matching baseline by as much as 7.5% and the large language model baseline by up to 2.74%. For detecting privacy concern topics from privacy reviews, our proposed algorithm achieves both better topic coherence and topic diversity than three strong topic modeling baselines, including LDA. Conclusion: Empirical evaluation results demonstrate the effectiveness of our approach in identifying privacy reviews and detecting user privacy concerns in app reviews. © 2025 Elsevier B.V., All rights reserved."
P018,"L., Wang, Lei; M., Wang, Mingchao; Y., Zhang, Yuanrong; J., Ma, Jian; H., Shao, Hongyu; Z., Chang, Zhixing",Automated Identification and Representation of System Requirements Based on Large Language Models and Knowledge Graphs,2025,Applied Sciences (Switzerland),1,10.3390/app15073502,"In the product design and manufacturing process, the effective management and representation of system requirements (SRs) are crucial for ensuring product quality and consistency. However, current methods are hindered by document ambiguity, weak requirement interdependencies, and limited semantic expressiveness in model-based systems engineering. To address these challenges, this paper proposes a prompt-driven integrated framework that synergizes large language models (LLMs) and knowledge graphs (KGs) to automate the visualization of SR text and structured knowledge extraction. Specifically, this paper introduces a template for information extraction tailored to arbitrary requirement documents, designed around five SysML-defined SR categories: functional requirements, interface requirements, performance requirements, physical requirements, and design constraints. By defining structured elements for each category and leveraging the GPT-4 model to extract key information from unstructured texts, the system can effectively extract and present the structured requirement information. Furthermore, the system constructs a knowledge graph to represent system requirements, visually illustrating the interdependencies and constraints between them. A case study applying this approach to Chapters 18–22 of the ‘Code for Design of Metro’ demonstrates the effectiveness of the proposed method in automating requirement representation, enhancing requirement traceability, and improving management. Moreover, a comparison of information extraction accuracy between GPT-4, GPT-3.5-turbo, BERT, and RoBERTa using the same dataset reveals that GPT-4 achieves an overall extraction accuracy of 84.76% compared to 79.05% for GPT-3.5-turbo and 59.05% for both BERT and RoBERTa. This proves the effectiveness of the proposed method in information extraction and provides a new technical pathway for intelligent requirement management. © 2025 Elsevier B.V., All rights reserved."
P019,"A.F., Subahi, Ahmad F.",Enhancing Software Sustainability: Leveraging Large Language Models to Evaluate Security Requirements Fulfillment in Requirements Engineering,2025,Systems,0,10.3390/systems13020114,"In the digital era, cybersecurity is integral for preserving national security, digital privacy, and social sustainability. This research emphasizes the role of non-functional equirements (NFRs) in developing secure software systems that enhance societal wellbeing by ensuring data protection, user privacy, and system robustness. Specifically, this study introduces a proof-of-concept approach by leveraging machine learning (ML) models to classify NFRs and identify security-related issues early in the software development lifecycle. Two experiments were conducted to assess the effectiveness of different models for binary and multi-class classification tasks. In Experiment 1, BERT-based models and artificial neural networks (ANNs) were fine-tuned to classify NFRs into security and non-security categories using a dataset of 803 statements. BERT-based models outperformed ANNs, achieving higher accuracy, precision, recall, and ROC-AUC scores, with hyperparameter tuning further enhancing the results. Experiment 2 assessed logistic regression (LR), a support vector machine (SVM), and XGBoost for the multi-class classification of security-related NFRs into seven categories. The SVM and XGBoost showed strong performance, achieving high precision and recall in specific categories. The findings demonstrate the effectiveness of advanced ML models in automating NFR classification, improving software security, and supporting social sustainability. Future work will explore hybrid approaches to enhance scalability and accuracy. © 2025 Elsevier B.V., All rights reserved."
P020,"N.D., Khan, Nek Dil; J.A., Khan, Javed Ali; J., Li, Jianqiang; T., Ullah, Tahir; Q., Zhao, Qing",Leveraging Large Language Model ChatGPT for enhanced understanding of end-user emotions in social media feedbacks,2025,Expert Systems with Applications,4,10.1016/j.eswa.2024.125524,"For software evolution, user feedback has become a meaningful way to improve applications. Recent studies show a significant increase in analyzing end-user feedback from various social media platforms for software evolution. However, less attention has been given to the end-user feedback for low-rating software applications. Also, such approaches are developed mainly on the understanding of human annotators who might have subconsciously tried for a second guess, questioning the validity of the methods. For this purpose, we proposed an approach that analyzes end-user feedback for low-rating applications to identify the end-user opinion types associated with negative reviews (an issue or bug). Also, we utilized Generative Artificial Intelligence (AI), i.e., ChatGPT, as an annotator and negotiator when preparing a truth set for the deep learning (DL) classifiers to identify end-user emotion. For the proposed approach, we first used the ChatGPT Application Programming Interface (API) to identify negative end-user feedback by processing 71853 reviews collected from 45 apps in the Amazon store. Next, a novel grounded theory is developed by manually processing end-user negative feedback to identify frequently associated emotion types, including anger, confusion, disgust, distrust, disappointment, fear, frustration, and sadness. Next, two datasets were developed, one with human annotators using a content analysis approach and the other using ChatGPT API with the identified emotion types. Next, another round is conducted with ChatGPT to negotiate over the conflicts with the human-annotated dataset, resulting in a conflict-free emotion detection dataset. Finally, various DL classifiers, including LSTM, BILSTM, CNN, RNN, GRU, BiGRU and BiRNN, are employed to identify their efficacy in detecting end-users emotions by preprocessing the input data, applying feature engineering, balancing the data set, and then training and testing them using a cross-validation approach. We obtained an average accuracy of 94%, 94%, 93%, 92%, 91%, 91%, and 85%, with LSTM, BILSTM, RNN, CNN, GRU, BiGRU and BiRNN, respectively, showing improved results with the truth set curated with human and ChatGPT. Using ChatGPT as an annotator and negotiator can help automate and validate the annotation process, resulting in better DL performances. © 2024 Elsevier B.V., All rights reserved."
P026,"A., Boudribila, Abderrahmane; A., Tajer, Abdelouahed; Z., Boulghasoul, Zakaria",Intelligent extraction of manufacturing system components from natural language using transformer models,2025,International Journal of Production Research,0,10.1080/00207543.2025.2554311,"Writing control programs for manufacturing systems is time-consuming and requires expert knowledge. Traditional methods, whether heuristic or formal, often fail to scale with the complexity and flexibility of modern production systems. A key challenge limiting progress is the lack of labelled data tailored to the domain of control logic. This gap hinders the development of AI systems capable of automating control program generation from natural language specifications. In this work, we introduce AutoFactory, a new dataset specifically designed for this task, along with AutoLabel-NER, a custom labelling tool that supports efficient annotation. We frame component extraction as a Named Entity Recognition problem and fine-tune several transformer-based models to evaluate their ability to detect relevant entities in requirement specifications. We benchmark thirteen transformer-based models on this dataset and compare their performance. Our results show that language models can detect control components accurately. To continue toward automation, we propose a pipeline that maps extracted components to predefined local control logic blocks. This pipeline generates IEC 61131-3 Structured Text code without manual programming. Our work creates the first benchmark and method for AI-assisted control program generation. It provides a solid foundation for future research in intelligent industrial automation. © 2025 Elsevier B.V., All rights reserved."
P027,"H., Han, Han; Z., Huang, Zheng; J., He, Jianhua; K., Chen, Kai",LERE: A Method Based on LLMs and Enhanced EARS in Requirements Engineering,2025,,0,10.1109/ICOSSE65712.2025.00022,"Normalized requirements permeate the entire requirements engineering and serve as a crucial foundation for the project. To improve the accuracy of requirements normalization in Chinese, in this study, we propose a method: LERE, which is based on LLMs and an enhanced EARS paradigm. The enhanced EARS is an optimized version of the original English EARS, incorporating elements from Chinese linguistic structures. This method employs a two-step processing prompt, with each step utilizing the enhanced EARS to perform structural element analysis on the input. The first step involves preprocessing the input, while the second step normalizes the output from the preprocessing stage by refining its syntactic structure and semantic expression. We have constructed a dataset consisting of 138 Chinese requirements, categorized into eight types of errors. The dataset and prompts have been made publicly available. Testing results on this dataset using Qwen2.5 and ChatGPT-4o show that the performance of LERE improved by approximately 20% compared to baseline. © 2025 Elsevier B.V., All rights reserved."
P028,"A., Florea, Alexandra; Ș.D., Achirei, Ștefan Daniel",Text Classification on Software and Hardware Requirements Using Natural Language Processing Techniques,2025,"SACI, IEEE International Symposium on Applied Computational Intelligence and Informatics",0,10.1109/SACI66288.2025.11030133,"This article presents a comparative study of text classification [1] models used in the automotive industries, using NLP (Natural Language Processing) [2]. The focus is placed on using the DistilBERT model [3] to classify requirements based on their functionality, with the goal of achieving a 90 % accuracy. Two significant experiments have been performed: the first compares performance on unprocessed and filtered data sets, while the second evaluates the impact of class distribution on various data sets. The results show that the model adjusted to a set of filtered data performs better than the model trained with initial data, with a final accuracy of 96%. Additionally, the model's performance is improved by balanced class distributions and data preprocessing methods like tokenisation and stopword removal. The study shows that early stopping mechanisms and filtering strategies improve generalisation and support minimise overfitting, which improves perspectives for the study of software and hardware requirement classification [4] using natural language processing. © 2025 Elsevier B.V., All rights reserved."
P032,"R., Batoool, Rizwana; A., Naseer, Ayesha",Functional and Non-functional Requirements Classification: A Comparative Evaluation of Pre-trained LLMs and ML Techniques,2025,,0,10.1109/ComTech65062.2025.11034464,"Identifying functional and non-functional requirements at an early phase is essential for project success. However, the requirements engineering community still lacks a comprehensive understanding of these requirements, which are often intertwined and expressed in natural language. Requirements classification is crucial for correctly extracting and organizing functional and non-functional requirements into specified categories. Automated classification reduces development costs, uncertainty, and misunderstanding. Machine learning (ML) and deep learning approaches have been applied for automatic classification in recent studies. This study conducts a comparative analysis by combining two publicly available PROMISE_exp and DOSSPRE datasets, which was classified as functional and non-functional classes of software requirements. First of all, we applied natural language processing (NLP) techniques to the requirements text to extract feature embeddings, followed by training four popular machine learning (ML) algorithms on these requirements. Inspired by the success of large language models (LLMs) in various tasks, we also fine-tuned four text-based pretrained (LLMs) and compared their performance with traditional ML models. Our empirical analysis shows that these models outperform traditional ML models on the combined dataset, offering developers an efficient method to detect and classify software requirements early. © 2025 Elsevier B.V., All rights reserved."
P042,"S.T.U., Shah, Syed Tauhid Ullah; M., Hussein, Mohamad; A., Barcomb, Ann; M., Moshirpour, Mohammad",From Inductive to Deductive: LLMs-Based Qualitative Data Analysis in Requirements Engineering,2025,CEUR Workshop Proceedings,0,,"Requirements Engineering (RE) is essential for developing complex and regulated software projects. Given the challenges in transforming stakeholder inputs into consistent software designs, Qualitative Data Analysis (QDA) provides a systematic approach to handling free-form data. However, traditional QDA methods are time-consuming and heavily reliant on manual effort. In this paper, we explore the use of Large Language Models (LLMs), including GPT-4, Mistral, and LLaMA-2, to improve QDA tasks in RE. Our study evaluates LLMs’ performance in inductive (zero-shot) and deductive (one-shot, few-shot) annotation tasks, revealing that GPT-4 achieves substantial agreement with human analysts in deductive settings, with Cohen’s Kappa scores exceeding 0.7, while zero-shot performance remains limited. Detailed, context-rich prompts significantly improve annotation accuracy and consistency, particularly in deductive scenarios, and GPT-4 demonstrates high reliability across repeated runs. These findings highlight the potential of LLMs to support QDA in RE by reducing manual effort while maintaining annotation quality. The structured labels automatically provide traceability of requirements and can be directly utilized as classes in domain models, facilitating systematic software design. © 2025 Elsevier B.V., All rights reserved."
P043,"V., Hippargi, Vibhashree; E., Kamsties, Erik; J., Naumann, Jürgen",Evaluating the Capabilities of LLMs in Traceability Maintenance for Automotive System and Software Requirements,2025,CEUR Workshop Proceedings,0,,"Various researchers have explored the potential of Large Language Models (LLMs) for several software engineering tasks, including design solution generation, coding, and test case creation. This paper presents five empirical studies performed on OpenAI’s ChatGPT-4o to analyze its performance to support different requirements engineering tasks related to requirements traceability. Using a dataset from an ongoing automotive project and industry experts’ assessments as ground truth, we evaluate ChatGPT-4o’s ability to assess trace link quality between system requirements, software requirements and test cases, to predict the trace links and also to analyze the quality of the requirements itself. We also tested ChatGPT-4o with an existing project ticket. Our findings in these studies indicate that ChatGPT-4o demonstrated strong performance, as evidenced by the metrics. These results suggest that ChatGPT-4o can be effectively integrated into daily industry practices as a support tool. The dataset is available on GitHub [3]. © 2025 Elsevier B.V., All rights reserved."
P047,"A., El-Hajjami, Abdelkarim; C., Salinesi, Camille",Synthline: A Product Line Approach for Synthetic Requirements Engineering Data Generation Using Large Language Models,2025,Lecture Notes in Business Information Processing,0,10.1007/978-3-031-92474-3_13,"While modern Requirements Engineering (RE) heavily relies on natural language processing and Machine Learning (ML) techniques, their effectiveness is limited by the scarcity of high-quality datasets. This paper introduces Synthline, a Product Line (PL) approach that leverages Large Language Models to systematically generate synthetic RE data for classification-based use cases. Through an empirical evaluation conducted in the context of using ML for the identification of requirements specification defects, we investigated both the diversity of the generated data and its utility for training downstream models. Our analysis reveals that while synthetic datasets exhibit less diversity than real data, they are good enough to serve as viable training resources. Moreover, our evaluation shows that combining synthetic and real data leads to substantial performance improvements. Specifically, hybrid approaches achieve up to 85% improvement in precision and a 2× increase in recall compared to models trained exclusively on real data. These findings demonstrate the potential of PL-based synthetic data generation to address data scarcity in RE. We make both our implementation and generated datasets publicly available to support reproducibility and advancement in the field. © 2025 Elsevier B.V., All rights reserved."
P048,"G., Xanthopoulou, Georgia; M.G., Siavvas, Miltiadis G.; I., Kalouptsoglou, Ilias; D.D., Kehagïas, Dionysios D.; D.K., Tzovaras, Dimitrios K.",Software Requirements Classification: From Bag-of-Words to Transformer,2025,Lecture Notes in Networks and Systems,0,10.1007/978-3-031-76459-2_35,"Automated classification of software requirements is valuable for software engineering. Recently, Natural Language Processing (NLP) and Machine Learning (ML) techniques have been utilized as an alternative to manual classification of requirements. In this study, we conduct a thorough empirical evaluation of several NLP methods utilized for efficient classification of software requirements. We focus both on the binary classification between functional and non-functional requirements (NFRs), and on the multi-class classification of the NFRs into specific categories, such as security, performance, usability, etc. For this purpose, we collected and enriched a large dataset of labeled software requirements, paying particular emphasis on security-related requirements. A wide range of NLP-based models were constructed and compared, raging from simple ML models that utilize the Bag-of-Words (BoW) technique for text representation to the more advanced Large Language Models (LLMs) that emerged recently. The results of our analysis demonstrated the ability of all the examined NLP-based models to provide highly accurate requirements classification both at binary and in multi-class setting, with Transformer-based models demonstrating the best predictive performance, thus revealing the benefits of transfer learning. © 2025 Elsevier B.V., All rights reserved."
P049,"C., Ge, Chuyan; T., Wang, Tiantian; X., Yang, Xiaotian; C., Treude, Christoph",Cross-Level Requirements Tracing Based on Large Language Models,2025,IEEE Transactions on Software Engineering,0,10.1109/TSE.2025.3572094,"Cross-level requirements traceability, linking high-level requirements (HLRs) and low-level requirements (LLRs), is essential for maintaining relationships and consistency in software development. However, the manual creation of requirements links necessitates a profound understanding of the project and entails a complex and laborious process. Existing machine learning and deep learning methods often fail to fully understand semantic information, leading to low accuracy and unstable performance. This paper presents the first approach for cross-level requirements tracing based on large language models (LLMs) and introduces a data augmentation strategy (such as synonym replacement, machine translation, and noise introduction) to enhance model robustness. We compare three fine-tuning strategies—LoRA, P-Tuning, and Prompt-Tuning—on different scales of LLaMA models (1.1B, 7B, and 13B). The fine-tuned LLMs exhibit superior performance across various datasets, including six single-project datasets, three cross-project datasets within the same domain, and one cross-domain dataset. Experimental results show that fine-tuned LLMs outperform traditional information retrieval, machine learning, and deep learning methods on various datasets. Furthermore, we compare the performance of GPT and DeepSeek LLMs under different prompt templates, revealing their high sensitivity to prompt design and relatively poor result stability. Our approach achieves superior performance, outperforming GPT-4o and DeepSeek-r1 by 16.27% and 16.8% in F-measure on cross-domain datasets. Compared to the baseline method that relies on prompt engineering, it achieves a maximum improvement of 13.8%. © 2025 Elsevier B.V., All rights reserved."
P052,"S.S., Ghaisas, Smita S.; A., Singhal, Anmol",Dealing with Data for RE: Mitigating Challenges While Using NLP and Generative AI,2025,,1,10.1007/978-3-031-73143-17,"Across the dynamic business landscape today, enterprises face an ever-increasing range of challenges. These include the constantly evolving regulatory environment, the growing demand for personalization within software applications and the heightened emphasis on governance. In response to these multifaceted demands, large enterprises have been adopting automation that spans from the optimization of core business processes to the enhancement of customer experiences. Indeed, Artificial Intelligence (AI) has emerged as a pivotal element of modern software systems. In this context, data plays an indispensable role. AI-centric software systems based on supervised learning and operating at an industrial scale require large volumes of training data to perform effectively. Moreover, the incorporation of generative AI has led to a growing demand for adequate evaluation benchmarks. Our experience in this field has revealed that the requirement for large datasets for training and evaluation introduces a host of intricate challenges. This book chapter explores the evolving landscape of Software Engineering (SE) in general, and Requirements Engineering (RE) in particular, in this era marked by AI integration. We discuss challenges that arise while integrating Natural Language Processing (NLP) and generative AI into enterprise-critical software systems. The chapter provides practical insights, solutions and examples to equip readers with the knowledge and tools necessary for effectively building solutions with NLP at their cores. We reflect on how these text data-centric tasks sit together with the traditional RE process. With this effort, we hope to engage students, faculty and industry researchers in a discussion that could lead to the identification of new and emerging text data-centric tasks relevant to RE. We also highlight new RE tasks that may be necessary for handling the increasingly important text data-centricity involved in developing software systems. © 2025 Elsevier B.V., All rights reserved."
P057,"R., Alharbi, Reham; J.D., Berardinis, Jacopo De; F., Grasso, Floriana; T.R., Payne, Terry R.; V.A.M., Tamma, Valentina A.M.",Characteristics and Desiderata for Competency Question Benchmarks,2025,CEUR Workshop Proceedings,0,,"Competency Questions (CQs) are essential in ontology engineering; they express an ontology’s functional requirements through natural language questions, offer crucial insights into an ontology’s scope, and are pivotal for various tasks, such as ontology reuse, testing, requirement specification, and pattern definition. Various approaches have emerged that make use of LLMs for the generation of CQs from different knowledge sources. However, comparative evaluations are hindered by differences in tasks, datasets and evaluation measures used. In this paper, we provide a set of desiderata for a benchmark of CQs, where we position state of the art approaches with respect to a categorisation of tasks, and highlight the main challenges hindering the definition of a community-based benchmark to support comparative studies. © 2025 Elsevier B.V., All rights reserved."
P058,"T., Hey, Tobias; D., Fuchß, Dominik; J., Keim, Jan; A., Koziolek, Anne",Requirements Traceability Link Recovery via Retrieval-Augmented Generation,2025,Lecture Notes in Computer Science,0,10.1007/978-3-031-88531-0_27,"[Context and Motivation] In software development, various interrelated artifacts are created. Access to information on the relation between these artifacts eases understanding of the system and enables tasks such as change impact and software reusability analyses. Manual trace link creation is labor-intensive and costly, and thus is often missing in projects. Automation could enhance the development and maintenance efficiency. [Question/Problem] Current methods for automatically recovering traceability links between different types of requirements do not achieve the necessary performance to be applied in practice, or require pre-existing links for machine learning. [Principal Ideas and Results] We propose to address this limitation by leveraging large language models (LLMs) with retrieval-augmented generation (RAG) for inter-requirements traceability link recovery. In an empirical evaluation on six benchmark datasets, we show that chain-of-thought prompting can be beneficial, open-source models perform comparably to proprietary ones, and that the approach can outperform state-of-the-art and baseline approaches. [Contribution] This work presents an approach for inter-requirements traceability link recovery using RAG and provides the first empirical evidence of its performance. © 2025 Elsevier B.V., All rights reserved."
P065,"F., Bozyigit, Fatma; T., Bardakci, Tolgahan; A., Khalilipour, Alireza; M., Challenger, Moharram; G.J., Ramackers, Guus J.; Ö., Babur, Önder; M.R., Chaudron, Michel R.V.",Generating domain models from natural language text using NLP: a benchmark dataset and experimental comparison of tools,2024,Software and Systems Modeling,8,10.1007/s10270-024-01176-y,"Software requirements specification describes users’ needs and expectations on some target system. Requirements documents are typically represented by unstructured natural language text. Such texts are the basis for the various subsequent activities in software development, such as software analysis and design. As part of software analysis, domain models are made that describe the key concepts and relations between them. Since the analysis process is performed manually by business analysts, it is time-consuming and may introduce mistakes. Recently, researchers have worked toward automating the synthesis of domain models from textual software requirements. Current studies on this topic have limitations in terms of the volume and heterogeneity of experimental datasets. To remedy this, we provide a curated dataset of software requirements to be utilized as a benchmark by algorithms that transform textual requirements documents into domain models. We present a detailed evaluation of two text-to-model approaches: one based on a large-language model (ChatGPT) and one building on grammatical rules (txt2Model). Our evaluation reveals that both tools yield promising results with relatively high F-scores for modeling the classes, attributes, methods, and relationships, with txt2Model performing better than ChatGPT on average. Both tools have relatively lower performance and high variance when it comes to the relation types. We believe our dataset and experimental evaluation pave to way to advance the field of automated model generation from requirements. © 2024 Elsevier B.V., All rights reserved."
P069,"A.E., Gärtner, Alexander Elenga; D., Göhlich, Dietmar",Automated requirement contradiction detection through formal logic and LLMs,2024,Automated Software Engineering,7,10.1007/s10515-024-00452-x,"This paper introduces ALICE (Automated Logic for Identifying Contradictions in Engineering), a novel automated contradiction detection system tailored for formal requirements expressed in controlled natural language. By integrating formal logic with advanced large language models (LLMs), ALICE represents a significant leap forward in identifying and classifying contradictions within requirements documents. Our methodology, grounded on an expanded taxonomy of contradictions, employs a decision tree model addressing seven critical questions to ascertain the presence and type of contradictions. A pivotal achievement of our research is demonstrated through a comparative study, where ALICE’s performance markedly surpasses that of an LLM-only approach by detecting 60% of all contradictions. ALICE achieves a higher accuracy and recall rate, showcasing its efficacy in processing real-world, complex requirement datasets. Furthermore, the successful application of ALICE to real-world datasets validates its practical applicability and scalability. This work not only advances the automated detection of contradictions in formal requirements but also sets a precedent for the application of AI in enhancing reasoning systems within product development. We advocate for ALICE’s scalability and adaptability, presenting it as a cornerstone for future endeavors in model customization and dataset labeling, thereby contributing a substantial foundation to requirements engineering. © 2024 Elsevier B.V., All rights reserved."
P070,"Z., Zhao, Zheng; H., Jiang, Hongxiang; R., Zhao, Ran; B., He, Bing",Emergence of A Novel Domain Expert: A Generative AI-based Framework for Software Function Point Analysis,2024,,0,10.1145/3691620.3695293,"Estimating software functional size is a crucial initial step before development, impacting costs and timelines. This involves applying standard Function Point Analysis (FPA) to the Software Requirements Specification (SRS). However, manual analysis by Function Point (FP) analysts during the splitting of FP entries from SRS remains inefficient and costly. To address this issue, for the first time, we propose an AI-based domain expert for FPA, named FPA-EX. It employs a large language model (LLM), intelligently extracts software FP entries from SRS, providing automated support to enhance efficiency. Specifically, we construct a multi-domain FPA dataset through collecting and annotating 778 question-answer pairs related to various SRS. Based on this dataset, we present a novel densely supervised fine-tuning (DSFT) on LLM, which performs entries-level optimization over the human augmented text, ensuring precise FPs outputs. Finally, we design a ConceptAct Promting (CAP) process for correct logical reasoning. Experiments demonstrate the superior performance of FPA-EX, particularly higher than GPT3.5 by 0.491 on F1 scores. Furthermore, in practical application, FPA-EX significantly enhances the productivity of FP analysts, contributing to a shift towards more intelligent work patterns. © 2024 Elsevier B.V., All rights reserved."
P074,"J., Acharya, Jagrit; G., Ginde, Gouri",Graph Neural Network vs. Large Language Model: A Comparative Analysis for Bug Report Priority and Severity Prediction,2024,,0,10.1145/3663533.3664042,"A vast number of incoming bug reports demand effective methods to identify priority and severity for bug triaging. With increased technological advancement, machine learning and deep learning have been extensively examined to address this problem. Although Large Language Models (LLMs) such as Fine-tuned BERT (early generation LLM) have proven to capture context in the underlying textual data, severity and priority prediction demand additional features for understanding the relationships with other bug reports. This work utilizes the graph-based approach to model the bug reports and their other attributes, such as component, product and bug type information. It utilizes the relational intelligence of Graph Neural Network (GNN) to address the prioritization and severity assessment of bug reports in the Bugzilla bug tracking system. Initial tests on the Mozilla project dataset indicate that a project-wise predictive approach using GNNs yields higher accuracy in determining the priority and severity of bug reports compared to LLMs across multiple Mozilla projects, contributing to a notable advancement in the automation of bug severity and priority prediction tasks. Specifically, GNNs demonstrated a remarkable improvement over LLMs, increasing the priority prediction accuracy by 37% & 30% and severity prediction accuracy by 43% & 30% for Core and Firefox projects, respectively. Overall, GNN outperformed the Fine-tuned BERT (LLM) in predicting priority and severity for all the Mozilla projects. © 2024 Elsevier B.V., All rights reserved."
P075,"M.I., Azeem, Muhammad Ilyas; S., Abualhaija, Sallam",A Multi-solution Study on GDPR AI-enabled Completeness Checking of DPAs,2024,Empirical Software Engineering,2,10.1007/s10664-024-10491-3,"Specifying legal requirements for software systems to ensure their compliance with the applicable regulations is a major concern of requirements engineering. Personal data which is collected by an organization is often shared with other organizations to perform certain processing activities. In such cases, the General Data Protection Regulation (GDPR) requires issuing a data processing agreement (DPA) which regulates the processing and further ensures that personal data remains protected. Violating GDPR can lead to huge fines reaching to billions of Euros. Software systems involving personal data processing must adhere to the legal obligations stipulated both at a general level in GDPR as well as the obligations outlined in DPAs highlighting specific business. In other words, a DPA is yet another source from which requirements engineers can elicit legal requirements. However, the DPA must be complete according to GDPR to ensure that the elicited requirements cover the complete set of obligations. Therefore, checking the completeness of DPAs is a prerequisite step towards developing a compliant system. Analyzing DPAs with respect to GDPR entirely manually is time consuming and requires adequate legal expertise. In this paper, we propose an automation strategy that addresses the completeness checking of DPAs against GDPR provisions as a text classification problem. Specifically, we pursue ten alternative solutions which are enabled by different technologies, namely traditional machine learning, deep learning, language modeling, and few-shot learning. The goal of our work is to empirically examine how these different technologies fare in the legal domain. We computed F<inf>2</inf> score on a set of 30 real DPAs. Our evaluation shows that best-performing solutions yield F<inf>2</inf> score of 86.7% and 89.7% are based on pre-trained BERT and RoBERTa language models. Our analysis further shows that other alternative solutions based on deep learning (e.g., BiLSTM) and few-shot learning (e.g., SetFit) can achieve comparable accuracy, yet are more efficient to develop. © 2024 Elsevier B.V., All rights reserved."
P078,"G., Rejithkumar, Gokul; P.R., Anish, Preethu Rose; J.S., Shukla, Jyoti S.; S.S., Ghaisas, Smita S.",Probing with Precision: Probing Question Generation for Architectural Information Elicitation,2024,,0,10.1145/3643666.3648577,"Software Requirements Specifications (SRS) often lack the necessary level of specificity required by software architects to make well-informed architectural decisions. This deficiency compels software architects to probe business analysts to collect more details pertinent to architectural requirements from the clients. In our previous work, we introduced Probing Question-flows (PQ-flows) that can assist business analysts to probe stakeholders and gather architecturally significant information for the creation of a more comprehensive SRS. Key limitations of our previous work were the manually created templatized PQ-flows and the mapping of PQ-flows to the software requirements based on standard Vector Space Model. In this study, we propose a Retrieval Augmented Generation (RAG) prompting framework to address these limitations. We conducted experiments using ChatGPT and Mistral-7B models. We present our findings utilizing human and automated evaluation metrics on a subset of the publicly available PUblic REquirements (PURE) dataset. © 2024 Elsevier B.V., All rights reserved."
P080,"D., Luitel, Dipeeka; S., Hassani, Shabnam; M., Sabetzadeh, Mehrdad",Improving requirements completeness: automated assistance through large language models,2024,Requirements Engineering,16,10.1007/s00766-024-00416-3,"Natural language (NL) is arguably the most prevalent medium for expressing systems and software requirements. Detecting incompleteness in NL requirements is a major challenge. One approach to identify incompleteness is to compare requirements with external sources. Given the rise of large language models (LLMs), an interesting question arises: Are LLMs useful external sources of knowledge for detecting potential incompleteness in NL requirements? This article explores this question by utilizing BERT. Specifically, we employ BERT’s masked language model to generate contextualized predictions for filling masked slots in requirements. To simulate incompleteness, we withhold content from the requirements and assess BERT’s ability to predict terminology that is present in the withheld content but absent in the disclosed content. BERT can produce multiple predictions per mask. Our first contribution is determining the optimal number of predictions per mask, striking a balance between effectively identifying omissions in requirements and mitigating noise present in the predictions. Our second contribution involves designing a machine learning-based filter to post-process BERT’s predictions and further reduce noise. We conduct an empirical evaluation using 40 requirements specifications from the PURE dataset. Our findings indicate that: (1) BERT’s predictions effectively highlight terminology that is missing from requirements, (2) BERT outperforms simpler baselines in identifying relevant yet missing terminology, and (3) our filter reduces noise in the predictions, enhancing BERT’s effectiveness for completeness checking of requirements. © 2024 Elsevier B.V., All rights reserved."
P089,"D., Guidotti, Dario; L., Pandolfo, Laura; T., Fanni, Tiziana; M.K., Zedda, Maria Katiuscia; L., Pulina, Luca",Translating Requirements in Property Specification Patterns using LLMs,2024,CEUR Workshop Proceedings,1,,"This paper introduces ReqH, an innovative tool designed to streamline the translation of natural language requirements into Property Specification Patterns. The tool leverages the capabilities of Large Language Models, which are renowned for their ability to comprehend and generate human-like text. ReqH aims to address the challenges of translating informal requirements into formal specifications, a process that is crucial in industrial contexts, particularly within safety and security-critical domains which demand rigorous formalisation to ensure the reliability and security of systems. We present some preliminary results from evaluating our methodology on a dataset of semi-automatically generated automotive requirements. The findings indicate that Large Language Models, when applied to this translation process, show significant potential for improving the accuracy and efficiency of requirement specification. © 2025 Elsevier B.V., All rights reserved."
P093,"Z., Wang, Zhipeng; C., Feng, Changxi; L., Liu, Longfei; G., Jiao, Guotao; P., Ye, Peng",The Application of LLMs in the Analysis and Modeling of Software Requirements,2024,,0,10.1109/QRS-C63300.2024.00151,"In modern software engineering, requirements analysis and modeling are key steps in requirements engineering, influencing the subsequent system design and implementation. Traditional requirements analysis and modeling methods usually involve multiple stakeholders such as requirements providers and analysts working together collaboratively and iteratively, requiring a significant amount of manpower. It is of great significance to reduce the burden on requirements providers and analysts and improve the efficiency of analysis and modeling. In existing work, some merely utilize knowledge repositories to provide more knowledge in support of analysis or modeling, while others employ natural language processing (NLP) techniques to automate the analysis or modeling process. However, neither has relieved the burden on requirements providers and analysts. The emergence of large language models(LLMs) has brought new possibilities to requirements analysis and modeling. LLMs can combine knowledge base processing and understanding of large amounts of natural language data, providing a new automated requirement analysis method. This paper proposes an automated requirements analysis and modeling method for Support Vector Machine(SVM) classifier and LLMs - SVM-LLMs. The method is deployed based on the intelligent requirement service platform - 'Wisdom Requirement Communication', which combines SVM and LLMs to effectively improve the accuracy of requirements analysis and significantly reduce the time in the software development process. © 2024 Elsevier B.V., All rights reserved."
P097,"Y., Xu, Yilongfei; J., Feng, Jincao; W., Miao, Weikai",Learning from Failures: Translation of Natural Language Requirements into Linear Temporal Logic with Large Language Models,2024,"IEEE International Conference on Software Quality, Reliability and Security, QRS",0,10.1109/QRS62785.2024.00029,"Formalization of intended requirements is indispensable when using formal methods in software development. However, translating Natural Language (NL) requirements into formal specifications, such as Linear Temporal Logic (LTL), is error-prone. Although Large Language Models (LLMs) offer the potential for automatically translating unstructured NL requirements to LTL formulas, general-purpose LLMs face two major problems: First, low accuracy in translation. Second, high cost of model training and tuning. To tackle these challenges, we propose a new approach that combines dynamic prompt generation with human-computer interaction to leverage LLM for an accurate and efficient translation of unstructured NL requirements to LTL formulas. Our approach consists of two techniques: 1) Dynamic Prompt Generation, which automatically generates the most appropriate prompts for translating the inquired NL requirements. 2) Interactive Prompt Evolution, which helps LLMs to learn from previous translation errors, i.e., erroneous formalizations are amended by users and added as new prompt fragments. Our approach achieves remarkable performance in publicly available datasets from two distinct domains, comprising 36 and 255,000 NL-LTL pairs, respectively. Without human interaction, our method achieves up to 94.4% accuracy. When our approach is extended to another domain, the accuracy improves from an initial 27% to 78% under interactive prompt evolution. © 2024 Elsevier B.V., All rights reserved."
P110,"K., Kolthoff, Kristian; F., Kretzer, Felix; C., Bartelt, Christian; A., Maedche, Alexander; S.P., Ponzetto, Simone Paolo",Interlinking User Stories and GUI Prototyping: A Semi-Automatic LLM-Based Approach,2024,Proceedings of the IEEE International Conference on Requirements Engineering,4,10.1109/RE59067.2024.00045,"Interactive systems are omnipresent today and the need to create graphical user interfaces (GUIs) is just as ubiq-uitous. For the elicitation and validation of requirements, GUI prototyping is a well-known and effective technique, typically employed after gathering initial user requirements represented in natural language (NL) (e.g., in the form of user stories). Un-fortunately, G UI prototyping often requires extensive resources, resulting in a costly and time-consuming process. Despite various easy-to-use prototyping tools in practice, there is often a lack of adequate resources for developing G UI prototypes based on given user requirements. In this work, we present a novel Large Language Model (LLM)-based approach providing assistance for validating the implementation of functional NL- based require-ments in a GUI prototype embedded in a prototyping tool. In particular, our approach aims to detect functional user stories that are not implemented in a G UI prototype and provides recommendations for suitable GUI components directly imple-menting the requirements. We collected requirements for existing GUIs in the form of user stories and evaluated our proposed validation and recommendation approach with this dataset. The obtained results are promising for user story validation and we demonstrate feasibility for the GUI component recommendations. © 2024 Elsevier B.V., All rights reserved."
P111,"N., Klievtsova, Nataliia; J., Mangler, Juergen; T., Kampik, Timotheus; S., Rinderle-Ma, Stefanie",Utilizing Process Models in the Requirements Engineering Process Through Model2Text Transformation,2024,Proceedings of the IEEE International Conference on Requirements Engineering,1,10.1109/RE59067.2024.00028,"With the advent of large language models (LLMs), requirements engineers have gained a powerful natural language processing tool to analyze, query, and validate a wide variety of textual artifacts, thus potentially supporting the whole re-quirements engineering process from requirements elicitation to management. However, the input for the requirements engineering process often encompasses a variety of potential information sources in various formats, especially graphical models such as process models. Hence, this work aims to contribute to the state of the art by assessing the feasibility of utilizing graphical process models and their textual representations in the requirements engineering process. In particular, we focus on the extraction of textual process descriptions from process models as i) input for the requirements engineering process and ii) documentation as the result of process-oriented requirements engineering. To this end, we explore, quantify, and compare traditional deterministic and LLM-based extraction methods where the latter includes GPT3, GPT3.5, GPT4, and LLAMA. The evaluation assesses output quality and information loss based on one data set. The results indicate that LLMs produce human-like process descriptions based on the predefined patterns, but apparently lack true comprehension of the process models. © 2024 Elsevier B.V., All rights reserved."
P116,"A.R., Preda, Anamaria Roberta; C., Mayr-Dorn, Christoph; A., Mashkoor, Atif; A.J., Egyed, Alexander James",Supporting High-Level to Low-Level Requirements Coverage Reviewing with Large Language Models,2024,,5,10.1145/3643991.3644922,"Refining high-level requirements into low-level ones is a common task, especially in safety-critical systems engineering. The objective is to describe every important aspect of the high-level requirement in a low-level requirement, ensuring a complete and correct implementation of the system's features. To this end, standards and regulations for safety-critical systems require reviewing the coverage of high-level requirements by all its low-level requirements to ensure no missing aspects.The challenge of supporting automatic reviews for requirements coverage originates from the distinct levels of abstraction between high-level and low-level requirements, their reliance on natural language, and the often different vocabulary used. The rise of Large Language Models (LLMs), trained on extensive text corpora and capable of contextualizing both high-level and low-level requirements, opens new avenues for addressing this challenge.This paper presents an initial study to explore the performance of LLMs in assessing requirements coverage. We employed GPT-3.5 and GPT-4 to analyze requirements from five publicly accessible data sets, determining their ability to detect if low-level requirements sufficiently address the corresponding high-level requirement. Our findings reveal that GPT-3.5, utilizing a zero-shot prompting strategy augmented with the prompt of explaining, correctly identifies complete coverage in four out of five evaluation data sets. Additionally, it exhibits an impressive 99.7% recall rate in accurately identifying instances where coverage is incomplete due to removing a single low-level requirement across our entire set of evaluation data.CCS CONCEPTS• Software and its engineering → Software creation and management; Designing software; Requirements analysis. © 2024 Elsevier B.V., All rights reserved."
P120,"J., Peer, Jordan; Y., Mordecai, Yaniv; Y., Reich, Yoram",NLP4ReF: Requirements Classification and Forecasting: From Model-Based Design to Large Language Models,2024,IEEE Aerospace Conference Proceedings,1,10.1109/AERO58975.2024.10521022,"We introduce Natural Language Processing for Requirement Forecasting (NLP4ReF), a model-based machine learning and natural language processing solution for enhancing the Requirements Engineering (RE) process. RE continues to face significant challenges and demands innovative approaches for process efficiency. Traditional RE methods relying on natural language struggle with incomplete, hidden, forgotten, and evolving requirements during and after the critical design review, risking project failures and setbacks. NLP4ReF tackles several key challenges: a) distinguishing between functional and non-functional requirements, b) classification of requirements by their respective system classes, and c) generation of unanticipated requirements to enhance project success. NLP4ReF employs a common natural language toolkit (NLTK) package and the recently-trending Chat-GPT. We tested NLP4ReF on PROMISE_exp, a pre-existing dataset with 1000 software requirements, and PROMISE_IoT, an enhanced dataset with 2000 software and IoT requirements. We validated NLP4ReF on a genuine IoT project. NLP4ReF swiftly generated dozens of new requirements, verified by a team of systems engineers, of which over 70% were crucial for project success. We found that GPT is superior in authentic requirement generation, while NLTK excels at requirement classification. NLP4ReF offers significant time saving, effort reduction, and improved future-proofing. Our model-based design approach provides a foundation for enhanced RE practices and future research in this domain. © 2024 Elsevier B.V., All rights reserved."
P121,"A., El-Hajjami, Abdelkarim; N., Fafin, Nicolas; C., Salinesi, Camille","Which AI Technique Is Better to Classify Requirements? An Experiment with SVM, LSTM, and ChatGPT",2024,CEUR Workshop Proceedings,0,,"Recently, Large Language Models like ChatGPT have demonstrated remarkable proficiency in various Natural Language Processing tasks. Their application in Requirements Engineering, especially in requirements classification, has gained increasing interest. This paper report an extensive empirical evaluation of two ChatGPT models, specifically gpt-3.5-turbo, and gpt-4 in both zero-shot and few-shot settings for requirements classification. The question arises as to how these models compare to traditional classification methods, specifically Support Vector Machine and Long Short-Term Memory. Based on five different datasets, our results show that there is no single best technique for all types of requirement classes. Interestingly, the few-shot setting has been found to be beneficial primarily in scenarios where zero-shot results are significantly low. © 2024 Elsevier B.V., All rights reserved."
P131,"Q., Motger, Quim Motger; A., Miaschi, Alessio Miaschi; F., Dell'Orletta, Felice Dell'Orletta; X., Franch, Xavier Franch; J., Marco, Jordi Marco",T-FREX: A Transformer-based Feature Extraction Method from Mobile App Reviews,2024,"Proceedings 2024 IEEE International Conference on Software Analysis Evolution and Reengineering, SANER",11,10.1109/SANER60148.2024.00030,"Mobile app reviews are a large-scale data source for software-related knowledge generation activities, including software maintenance, evolution and feedback analysis. Effective extraction of features (i.e., functionalities or characteristics) from these reviews is key to support analysis on the acceptance of these features, identification of relevant new feature requests and prioritization of feature development, among others. Traditional methods focus on syntactic pattern-based approaches, typically context-agnostic, evaluated on a closed set of apps, difficult to replicate and limited to a reduced set and domain of apps. Mean-while, the pervasiveness of Large Language Models (LLMs) based on the Transformer architecture in software engineering tasks lays the groundwork for empirical evaluation of the performance of these models to support feature extraction. In this study, we present T-FREX, a Transformer-based, fully automatic approach for mobile app review feature extraction. First, we collect a set of ground truth features from users in a real crowdsourced software recommendation platform and transfer them automatically into a dataset of app reviews. Then, we use this newly created dataset to fine-tune multiple LLMs on a named entity recognition task under different data configurations. We assess the performance of T-FREX with respect to this ground truth, and we complement our analysis by comparing T-FREX with a baseline method from the field. Finally, we assess the quality of new features predicted by T-FREX through an external human evaluation. Results show that T-FREX outperforms on average the traditional syntactic-based method, especially when discovering new features from a domain for which the model has been fine-tuned. © 2024 IEEE."
P147,"J., Wei, Jialiang; A.L., Courbis, Anne Lise; T., Lambolais, Thomas; B., Xu, Binbin; P.L., Bernard, Pierre Louis; G., Dray, Gérard",Zero-shot Bilingual App Reviews Mining with Large Language Models,2023,,8,10.1109/ICTAI59109.2023.00135,"App reviews from app stores are crucial for improving software requirements. A large number of valuable reviews are continually being posted, describing software problems and expected features. Effectively utilizing user reviews necessitates the extraction of relevant information, as well as their subsequent summarization. Due to the substantial volume of user reviews, manual analysis is arduous. Various approaches based on natural language processing (NLP) have been proposed for automatic user review mining. However, the majority of them requires a manually crafted dataset to train their models, which limits their usage in real-world scenarios. In this work, we propose Mini-BAR, a tool that integrates large language models (LLMs) to perform zero-shot mining of user reviews in both English and French. Specifically, Mini-BAR is designed to (i) classify the user reviews, (ii) cluster similar reviews together, (iii) generate an abstractive summary for each cluster and (iv) rank the user review clusters. To evaluate the performance of Mini-BAR, we created a dataset containing 6,000 English and 6,000 French annotated user reviews and conducted extensive experiments. Preliminary results demonstrate the effectiveness and efficiency of Mini-BAR in requirement engineering by analyzing bilingual app reviews. © 2024 Elsevier B.V., All rights reserved."
P148,"S., Arulmohan, Sathurshan; M.J., Meurs, Marie Jean; S., Mosser, Sébastien",Extracting Domain Models from Textual Requirements in the Era of Large Language Models,2023,,33,10.1109/MODELS-C59198.2023.00096,"Requirements Engineering is a critical part of the software lifecycle, describing what a given piece of software will do (functional) and how it will do it (non-functional). Requirements documents are often textual, and it is up to software engineers to extract the relevant domain models from the text, which is an error-prone and time-consuming task. Considering the recent attention gained by Large Language Models (LLMs), we explored how they could support this task. This paper investigates how such models can be used to extract domain models from agile product backlogs and compare them to (i) a state-of-practice tool as well as (ii) a dedicated Natural Language Processing (NLP) approach, on top of a reference dataset of 22 products and 1, 679 user stories. Based on these results, this paper is a first step towards using LLMs and/or tailored NLP to support automated requirements engineering thanks to model extraction using artificial intelligence. © 2024 Elsevier B.V., All rights reserved."
P153,"F., Cruciani, Federico; S.J., Moore, Samuel J.; C.D., Nugent, Chris D.",Comparing general purpose pre-trained Word and Sentence embeddings for Requirements Classification,2023,CEUR Workshop Proceedings,1,,"The recent evolution of NLP has enriched the set of DL-based approaches to include a number of general-purpose Large Language Models (LLMs). Whereas new models have been proven useful for generic text handling, their applicability to domain-specific NLP tasks still remains doubtful, particularly because of the limited amount of dataset available in certain domains, such as Requirements Engineering. In this study, different pre-trained embeddings were tested in three requirements classification tasks, in search of a tradeoff between accuracy and computational complexity. The best F1-score results were obtained with BERT (90.36% and 84.23%), with DistilBERT identified as optimal tradeoff (90.28% and 82.61%). © 2023 Elsevier B.V., All rights reserved."